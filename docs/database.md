# Database.py
The file provides the `Database` object which handles safely reading and writing objects to an internal array of `CrawledURL` objects.

## CrawledURL object
The `CrawledURL` object provides some key information about a webpage. It contains three values:

    1. The url of the website we care about
    2. A UTC timestamp of when the object was created
        -> Used for determining which objects have stale data.
    3. A relevance dictionary which maps keywords and their relevance for that page.

This defined structure is required to use the msgspec msgpack to create the final database object.

`array_like=True` means that the fields of the object are stored in the array without the headers, which
saves space in the final output object.

These structures are what should be generated by the crawler threads.

## DatabaseStruct object
This combines the List of CrawledURL objects with a hash_table which provides O(1) lookups into the Database.data array. This Struct is what the database actually stores into a file and it is spilt apart when running the `Database.decode_file` function and is generated when writing the object to a file.
    
## Database object
This object handles providing functions for interacting with the database as well as successfully decoding and encoding the data into msgpack structures that can be saved to a file.

### Getting Started
The `Database` object should be instantiated like this:

    db = Database(input_file="test_database.msgpack")

Where it is given an input_file to attempt to read. If the file doesn't exist it will create an empty 
internal array. If it is unable to decode the file it will mark it read-only so that it doesn't overwrite
the existing file with the empty array from the failure to decode. This should only happen if the structure
of the database file changes. The object provides a separate `output_file` flag which can be set 
independently from the input_file but defaults to the value of `input_file` if none is given.

The `Database` object has a `cleanup` function which handles cleanly writing to the disk and must be called 
in a `finally` block to ensure that it gets updated successfully even when the program recieves a signal to
stop. See the below example for a main file outline:

    # Database uses input_file for output file when output_file doesn't
    # exist yet.
    db = Database(input_file="test_database.msgpack") # test_*.msgpack is in .gitignore
    try:
        main(db)
    finally:
        # Ensure we always save our state before closing the program.
        db.cleanup()
    
### Adding to the Database
The `Database.add` function expects a `CrawledURL` object. To create this object use the 
`Database.build_crawled_url()` function which takes a URL string and a dictionary of relevance
information then call `Database.add`:

    rel = {"tacos": 33, "networking":55, "blah":253, "data":6, "45":3}
    crawled = db.build_crawled_url("testurl", rel)
    db.add(crawled)

This function has an optional parameter called `force_add` which will overwrite the given entry with the same url.

### Getting a sorted list from the Database based on a keyword
The `Database.urls_with_keyword` function provides this functionality and will return an array of tuples with the
urls and their relevance sorted from highest to lowest relevance.

    array = db.urls_with_keyword("tacos")
    print(array)

    # These are generic random strings instead of URLs
    [('Y6YLGSW00FJK', 999999), ('SFUMNW7XAWSG', 999998), ('6RE81H7J2LHU', 999997), ...]

### Find stale URLs
The `Database.stale_urls` function returns an array of url strings that should be recrawled and their data should be updated.

This should be paired with the `force_add=True` parameter when adding the new data back to the database.

    urls_to_update = db.stale_urls
    for url in urls_to_update:
        relevance_dict = <function to get relevance>
        crawled = db.build_crawled_url(url, relevance_dict)
        db.add(crawled, force_add=True)
